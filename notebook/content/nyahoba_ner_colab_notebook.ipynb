{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4caf86ff",
      "metadata": {
        "id": "4caf86ff"
      },
      "source": [
        "# Nya-Hoba NER — Colab Notebook\n",
        "\n",
        "**Named Entity Recognition for Low-Resource African Languages: A Transformer-Based Case Study on Nya-Hoba**\n",
        "\n",
        "**Owner:** Chahyaandida Ishaya\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is prepared to run in Google Colab. Before running heavy training cells, set `Runtime -> Change runtime type -> GPU`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb01cb3",
      "metadata": {
        "id": "dbb01cb3"
      },
      "source": [
        "## Project Objectives\n",
        "\n",
        "- **General Objective:** Design and evaluate a transformer-based NER system for Nya-Hoba.\n",
        "- **Specific Objectives:**\n",
        "  1. Collect, clean, and annotate a Nya-Hoba text corpus for NER tasks.\n",
        "  2. Develop baseline NER models using traditional machine learning approaches for benchmarking.\n",
        "  3. Fine-tune transformer-based models for NER on Nya-Hoba.\n",
        "  4. Evaluate model performance using Precision, Recall, and F1-score.\n",
        "  5. Release an open-source dataset and pre-trained models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb998e09",
      "metadata": {
        "id": "eb998e09"
      },
      "source": [
        "## 1. Setup\n",
        "Run the code cell to install required packages. On Colab this may take a few minutes.\n",
        "\n",
        "You may want to manually install a CUDA-compatible `torch` build if you intend to use GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eaa0c52d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaa0c52d",
        "outputId": "7c719dac-ddef-4607-c82d-e765c10fc845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installed packages. Check versions:\n",
            "sklearn 1.6.1\n",
            "pandas 2.2.2\n",
            "transformers 4.56.1\n",
            "torch 2.8.0+cu126 CUDA: False\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q scikit-learn sklearn-crfsuite pandas joblib transformers datasets seqeval torch\n",
        "print('Installed packages. Check versions:')\n",
        "import sklearn, pandas, joblib, transformers\n",
        "import torch\n",
        "print('sklearn', sklearn.__version__)\n",
        "print('pandas', pandas.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('torch', torch.__version__, 'CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Generation & Cleaning**\n",
        " This stage expands small seed lists of entity words ( names, times, animals, and locations) into larger synthetic datasets through controlled randomization and pattern-based augmentation."
      ],
      "metadata": {
        "id": "CZ1hkw9QLpCO"
      },
      "id": "CZ1hkw9QLpCO"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "#Making directory for data\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "print('Your Data is located in /content/data/dataset.conll')\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 1. Fixed lists\n",
        "# ---------------------------\n",
        "time_words = [\n",
        "    \"Zǝkǝu\", \"Sakana\", \"ǝna\", \"ǝhna\", \"Pǝshinda\", \"Pishinda\",\n",
        "    \"Fer pǝchingǝ zekeu\", \"Pǝr pǝchingǝ zekeu\", \"Pǝchi\", \"Hya\"\n",
        "]\n",
        "\n",
        "animal_words = [\n",
        "    \"Kwa\", \"Mabǝlang\", \"Tǝga\", \"Gwanba\", \"Ha'l\", \"Dlǝgwam\", \"Thla\",\n",
        "    \"Kǝtǝn\", \"Chiwar\", \"Lǝvari\", \"Mapǝla'u\", \"Litsa\"\n",
        "]\n",
        "\n",
        "person_words_fixed = [\n",
        "    \"Chahyaandida\", \"Chabiya\", \"Hyellama\", \"Hyelnaya\", \"Wandiya\", \"Hyel\", \"Yesu\",\n",
        "    \"Chataimada\", \"Chatramada\", \"Nanunaya\", \"Mapida\", \"Shimbal\", \"Chai\",\n",
        "    \"Hyellachardati\", \"Hyellachardati\", \"Wamanyi\", \"Miyaninyi\", \"Miyakindahyelni\", \"Miyaninyi\"\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 2. Synthetic PERSON names\n",
        "# ---------------------------\n",
        "base_names = [\n",
        "    \"Abubakar\", \"Ibrahim\", \"Musa\", \"Usman\", \"Kabiru\", \"Bello\", \"Suleiman\",\n",
        "    \"Ahmad\", \"Aliyu\", \"Shehu\", \"Aminu\", \"Habiba\", \"Fatima\", \"Aisha\", \"Zainab\", \"Hauwa\",\n",
        "    \"Ruqayya\", \"Maryam\", \"Khadija\", \"Sa'adatu\", \"Yakubu\", \"Ismaila\", \"Nasiru\", \"Idris\",\n",
        "    \"John\", \"Paul\", \"Peter\", \"James\", \"Joseph\", \"Stephen\", \"Samuel\",\n",
        "    \"David\", \"Daniel\", \"Thomas\", \"Andrew\", \"Philip\", \"Simon\", \"Nathaniel\",\n",
        "    \"Grace\", \"Joyce\", \"Ruth\", \"Esther\", \"Naomi\", \"Sarah\", \"Deborah\",\n",
        "    \"Ndyako\", \"Pwakina\", \"Gargam\", \"Kwada\", \"Tizhe\", \"Lazarus\", \"Kwapre\",\n",
        "    \"Nzoka\", \"Jauro\", \"Birma\", \"Fwa\", \"Tumba\", \"Dlama\", \"Nuhu\", \"Zira\", \"Bitrus\",\n",
        "    \"Vandi\", \"Nggada\", \"Gimba\", \"Danjuma\"\n",
        "]\n",
        "\n",
        "prefixes = [\"Alhaji\", \"Malam\", \"Doctor\", \"Pastor\", \"Chief\", \"Prince\", \"Princess\", \"Rev\"]\n",
        "suffixes = [\"Abubakar\", \"Musa\", \"Ibrahim\", \"Aliyu\", \"Yakubu\", \"Bitrus\", \"Danjuma\", \"Zira\", \"Vandi\", \"Nuhu\"]\n",
        "syllables = [\"Nga\", \"Fwa\", \"Tiz\", \"Lam\", \"Bok\", \"Ngu\", \"Pwa\", \"Kiri\", \"Shaf\", \"Loru\", \"Baga\", \"Dla\", \"Hoba\", \"Zar\", \"Yam\", \"Kwada\"]\n",
        "\n",
        "def make_variants(base_list, prefixes, suffixes, syllables, target=2000, max_attempts=20000):\n",
        "    items = set(base_list)\n",
        "    attempts = 0\n",
        "    while len(items) < target and attempts < max_attempts:\n",
        "        r = random.random()\n",
        "        if r < 0.3 and prefixes:\n",
        "            new = random.choice(prefixes) + \" \" + random.choice(base_list)\n",
        "        elif r < 0.6 and suffixes:\n",
        "            new = random.choice(base_list) + \" \" + random.choice(suffixes)\n",
        "        elif r < 0.8 and syllables:\n",
        "            new = random.choice(syllables) + random.choice(syllables)\n",
        "        else:\n",
        "            new = random.choice(base_list) + \" \" + random.choice(base_list)\n",
        "        items.add(new)\n",
        "        attempts += 1\n",
        "\n",
        "    # Fill with duplicates if still short\n",
        "    items = list(items)\n",
        "    while len(items) < target:\n",
        "        items.append(random.choice(items))\n",
        "    return items[:target]\n",
        "\n",
        "random.seed(2025)\n",
        "all_person_names = make_variants(base_names + person_words_fixed, prefixes, suffixes, syllables, 2000)\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 2b. Expand TIME and ANIMAL with variants to 2000\n",
        "# ---------------------------\n",
        "time_prefixes = [\"Early\", \"Late\", \"Mid\", \"Pre\", \"Post\"]\n",
        "time_suffixes = [\"time\", \"hour\", \"day\", \"night\", \"season\"]\n",
        "time_syllables = [\"Zi\", \"Sa\", \"Na\", \"Ku\", \"Lo\", \"Mi\", \"Ta\"]\n",
        "\n",
        "animal_prefixes = [\"Wild\", \"Big\", \"Little\", \"Young\", \"Old\"]\n",
        "animal_suffixes = [\"beast\", \"cub\", \"ling\", \"hunter\", \"creature\"]\n",
        "animal_syllables = [\"Ka\", \"Mo\", \"La\", \"Ti\", \"Ro\", \"Zu\", \"Ba\"]\n",
        "\n",
        "all_time_words = make_variants(time_words, time_prefixes, time_suffixes, time_syllables, 2000)\n",
        "all_animal_words = make_variants(animal_words, animal_prefixes, animal_suffixes, animal_syllables, 2000)\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 3. Location generator\n",
        "# ---------------------------\n",
        "base_places = [\n",
        "    \"Yola\", \"Jimeta\", \"Numan\", \"Ganye\", \"Gombi\", \"Hong\", \"Mubi\", \"Michika\", \"Madagali\",\n",
        "    \"Maiha\", \"Fufore\", \"Song\", \"Demsa\", \"Guyuk\", \"Jada\", \"Lamurde\", \"Mayo-Belwa\",\n",
        "    \"Shelleng\", \"Toungo\", \"Pella\", \"Uba\", \"Dirma\", \"Holma\", \"Kala'a\", \"Garkida\",\n",
        "    \"Borrong\", \"Mayo-Lope\", \"Shuwa\", \"Mayo-Balewa\", \"River Benue\", \"Mayo Ine\",\n",
        "    \"Mayo Nguli\", \"Mayo Sanzu\", \"Kiri Dam\", \"Mandara Mountains\", \"Zumo Hill\", \"Fali Hills\"\n",
        "]\n",
        "\n",
        "prefixes_loc = [\"New\", \"Old\", \"Upper\", \"Lower\", \"North\", \"South\", \"East\", \"West\", \"Mayo\", \"Wuro\", \"Gidan\", \"Bari\"]\n",
        "suffixes_loc = [\"Gari\", \"Ward\", \"Hill\", \"Village\", \"Settlement\", \"Bridge\", \"Camp\", \"Market\", \"River\", \"Valley\", \"Peak\", \"Forest\", \"Reserve\", \"Dam\"]\n",
        "syllables_loc = [\"Kwa\", \"Ngu\", \"Mayo\", \"Zar\", \"Kiri\", \"Wuro\", \"Tula\", \"Nguwa\", \"Ganye\", \"Song\", \"Lam\", \"Mubi\", \"Pella\", \"Hoba\", \"Beli\", \"Tambo\", \"Shaf\", \"Loru\", \"Baga\", \"Zumo\"]\n",
        "\n",
        "all_places = make_variants(base_places, prefixes_loc, suffixes_loc, syllables_loc, 2000)\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 4. Annotation helper\n",
        "# ---------------------------\n",
        "def make_annotation(word, label):\n",
        "    return {\n",
        "        \"data\": {\"text\": word},\n",
        "        \"annotations\": [{\n",
        "            \"result\": [{\n",
        "                \"value\": {\n",
        "                    \"start\": 0,\n",
        "                    \"end\": len(word),\n",
        "                    \"text\": word,\n",
        "                    \"labels\": [label]\n",
        "                },\n",
        "                \"from_name\": \"label\",\n",
        "                \"to_name\": \"text\",\n",
        "                \"type\": \"labels\"\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "\n",
        "# Build datasets\n",
        "time_tasks = [make_annotation(w, \"TIME\") for w in all_time_words]          # expanded 2000\n",
        "animal_tasks = [make_annotation(w, \"ANIMAL\") for w in all_animal_words]    # expanded 2000\n",
        "person_tasks = [make_annotation(w, \"PERSON\") for w in all_person_names]    # expanded 2000\n",
        "location_tasks = [make_annotation(loc, \"LOCATION\") for loc in all_places]  # expanded 2000\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 5. Merge datasets\n",
        "# ---------------------------\n",
        "merged = time_tasks + animal_tasks + person_tasks + location_tasks\n",
        "\n",
        "with open(\"/content/data/merged_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(merged, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Saved {len(merged)} tasks -> merged_dataset.json\")\n",
        "print(f\"  TIME: {len(time_tasks)}\")\n",
        "print(f\"  ANIMAL: {len(animal_tasks)}\")\n",
        "print(f\"  PERSON: {len(person_tasks)}\")\n",
        "print(f\"  LOCATION: {len(location_tasks)}\")\n"
      ],
      "metadata": {
        "id": "rcjmh9w0Llci"
      },
      "id": "rcjmh9w0Llci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Data Annotation**\n",
        " The generated data is then annotated with entity labels and merged into a unified dataset, ensuring sufficient volume and diversity for NER model training while maintaining consistency and quality."
      ],
      "metadata": {
        "id": "d6wChJmxL30P"
      },
      "id": "d6wChJmxL30P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your merged dataset\n",
        "with open(\"/content/data/merged_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "conll_lines = []\n",
        "\n",
        "for task in data:\n",
        "    text = task[\"data\"][\"text\"]\n",
        "    anns = task[\"annotations\"][0][\"result\"] if task[\"annotations\"] else []\n",
        "\n",
        "    # Start with \"O\" for each token\n",
        "    tokens = text.split()\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    for ann in anns:\n",
        "        value = ann[\"value\"]\n",
        "        start = value[\"start\"]\n",
        "        end = value[\"end\"]\n",
        "        label = value[\"labels\"][0]\n",
        "\n",
        "        # Find which tokens are covered by this annotation\n",
        "        covered = []\n",
        "        running_index = 0\n",
        "        for i, tok in enumerate(tokens):\n",
        "            token_start = running_index\n",
        "            token_end = running_index + len(tok)\n",
        "            if token_end > start and token_start < end:\n",
        "                covered.append(i)\n",
        "            running_index = token_end + 1  # +1 for space\n",
        "\n",
        "        # Assign BIO tags\n",
        "        for j, idx in enumerate(covered):\n",
        "            if j == 0:\n",
        "                labels[idx] = \"B-\" + label\n",
        "            else:\n",
        "                labels[idx] = \"I-\" + label\n",
        "\n",
        "    # Append tokens with tags\n",
        "    for tok, lab in zip(tokens, labels):\n",
        "        conll_lines.append(f\"{tok} {lab}\")\n",
        "    conll_lines.append(\"\")  # Sentence boundary\n",
        "\n",
        "# Save to file\n",
        "with open(\"/content/data/dataset.conll\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(conll_lines))\n",
        "\n",
        "print(\"✅ Exported to dataset.conll in CoNLL format\")\n"
      ],
      "metadata": {
        "id": "AfqZC-IVLwpK"
      },
      "id": "AfqZC-IVLwpK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "445151ac",
      "metadata": {
        "id": "445151ac"
      },
      "source": [
        "## 4. Parse CoNLL & Prepare JSONL\n",
        "This cell parses the CoNLL file (token per line, tag in last column) and saves a JSONL to `/content/prepared/data.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6218bf0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6218bf0d",
        "outputId": "7a337cf1-b5f8-4852-dcd8-e98551e88673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: 8000\n",
            "Tokens (total): 15843\n",
            "Label set: ['B-ANIMAL', 'B-LOCATION', 'B-PERSON', 'B-TIME', 'I-ANIMAL', 'I-LOCATION', 'I-PERSON', 'I-TIME']\n",
            "Saved prepared JSONL to /content/prepared/data.jsonl\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json, os\n",
        "from collections import Counter\n",
        "\n",
        "conll_path = Path('/content/data/dataset.conll')\n",
        "if not conll_path.exists():\n",
        "    raise FileNotFoundError('dataset.conll not found at /content/data/dataset.conll. Please upload it first.')\n",
        "\n",
        "def read_conll(path):\n",
        "    sentences = []\n",
        "    tokens, tags = [], []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    sentences.append((tokens, tags))\n",
        "                    tokens, tags = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            token = parts[0]\n",
        "            tag = parts[-1] if len(parts) > 1 else 'O'\n",
        "            tokens.append(token); tags.append(tag)\n",
        "        if tokens:\n",
        "            sentences.append((tokens, tags))\n",
        "    return sentences\n",
        "\n",
        "sentences = read_conll(conll_path)\n",
        "num_sentences = len(sentences)\n",
        "num_tokens = sum(len(s[0]) for s in sentences)\n",
        "labels = Counter()\n",
        "for toks, tgs in sentences:\n",
        "    labels.update(tgs)\n",
        "\n",
        "print('Sentences:', num_sentences)\n",
        "print('Tokens (total):', num_tokens)\n",
        "print('Label set:', sorted(labels.keys()))\n",
        "\n",
        "os.makedirs('/content/prepared', exist_ok=True)\n",
        "with open('/content/prepared/data.jsonl', 'w', encoding='utf-8') as outf:\n",
        "    for toks, tgs in sentences:\n",
        "        outf.write(json.dumps({'tokens': toks, 'tags': tgs}, ensure_ascii=False) + '\\n')\n",
        "print('Saved prepared JSONL to /content/prepared/data.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0115848",
      "metadata": {
        "id": "f0115848"
      },
      "source": [
        "### Sample annotated sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ca22b28a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca22b28a",
        "outputId": "4f1ee368-304f-4084-ba24-b00f9b5c4c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 -> SaZi/B-TIME\n",
            "2 -> NaKu/B-TIME\n",
            "3 -> ZiSa/B-TIME\n",
            "4 -> Early/B-TIME Zǝkǝu/I-TIME\n",
            "5 -> ZiNa/B-TIME\n",
            "6 -> Pre/B-TIME Pishinda/I-TIME\n",
            "7 -> Sakana/B-TIME Zǝkǝu/I-TIME\n",
            "8 -> Sakana/B-TIME season/I-TIME\n",
            "9 -> Hya/B-TIME hour/I-TIME\n",
            "10 -> Fer/B-TIME pǝchingǝ/I-TIME zekeu/I-TIME night/I-TIME\n"
          ]
        }
      ],
      "source": [
        "# print first 10 samples\n",
        "import json, itertools\n",
        "with open('/content/prepared/data.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(itertools.islice(f, 10), 1):\n",
        "        d = json.loads(line)\n",
        "        print(i, '->', ' '.join([f\"{t}/{tg}\" for t,tg in zip(d['tokens'], d['tags'])]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf7454d3",
      "metadata": {
        "id": "cf7454d3"
      },
      "source": [
        "## 5. Baseline: CRF Model\n",
        "Train a CRF baseline using `sklearn-crfsuite`. This step is fast and useful for benchmarking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dbc92629",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbc92629",
        "outputId": "4a0e1b65-c6ef-4f7f-8c04-fd06db13b997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      B-TIME     1.0000    1.0000    1.0000       425\n",
            "      I-TIME     1.0000    1.0000    1.0000       537\n",
            "    B-PERSON     0.9831    0.8816    0.9296       397\n",
            "    I-PERSON     1.0000    1.0000    1.0000       335\n",
            "  B-LOCATION     0.8912    0.9850    0.9357       399\n",
            "  I-LOCATION     1.0000    1.0000    1.0000       416\n",
            "    B-ANIMAL     1.0000    0.9974    0.9987       379\n",
            "    I-ANIMAL     1.0000    1.0000    1.0000       306\n",
            "\n",
            "    accuracy                         0.9831      3194\n",
            "   macro avg     0.9843    0.9830    0.9830      3194\n",
            "weighted avg     0.9843    0.9831    0.9831      3194\n",
            "\n",
            "Saved CRF model to /content/prepared/crf_nyahoba.joblib\n"
          ]
        }
      ],
      "source": [
        "# CRF baseline training\n",
        "import json\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "data = [json.loads(line) for line in open('/content/prepared/data.jsonl', encoding='utf-8')]\n",
        "tokens = [d['tokens'] for d in data]\n",
        "tags = [d['tags'] for d in data]\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "X = [sent2features(s) for s in tokens]\n",
        "y = tags\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=200)\n",
        "crf.fit(X_train, y_train)\n",
        "y_pred = crf.predict(X_test)\n",
        "labels = [l for l in crf.classes_ if l != 'O']\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=labels, digits=4))\n",
        "\n",
        "joblib.dump(crf, '/content/prepared/crf_nyahoba.joblib')\n",
        "print('Saved CRF model to /content/prepared/crf_nyahoba.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f10c96",
      "metadata": {
        "id": "79f10c96"
      },
      "source": [
        "## 6. Transformer Fine-tuning (Hugging Face)\n",
        "Fine-tune a transformer for token classification. This cell uses Hugging Face `Trainer`. **Requires GPU** for reasonable speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567,
          "referenced_widgets": [
            "f75a5ac82ffe4a22a533e4ccf8bea560",
            "db24d48f370b4d4e9b92eb9322d882cd",
            "d9fe824d85ab41a58737325e7f9ffbf3",
            "dd2ed684fdee49879a57fe0f39e7f02c",
            "f2c9c17d733d4ef89b07af34457e5bd2",
            "70ed8add509e4103b94cb9b64bb76206",
            "9d7b85c944534646a69f679b6671c828",
            "d0ec36e2ea5c41229e1926741a238dfe",
            "0db8ceef719a4b948d46a87ccc5ee440",
            "e030f64252f94d3ea68a80e758ef6999",
            "64a3ce25324c426a9c06deeb4b0a9247"
          ]
        },
        "id": "817afc7c",
        "outputId": "c5562428-64f0-4ce1-cb5d-61c071f23140"
      },
      "source": [
        "# Rerun the transformer fine-tuning template\n",
        "# (This may be slow on Colab free tier and requires GPU for reasonable speed)\n",
        "\n",
        "# Ensure you have a GPU runtime enabled:\n",
        "# Go to Runtime -> Change runtime type -> GPU\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "import json, os\n",
        "\n",
        "MODEL_NAME = \"xlm-roberta-base\"  # change to an Afro model if you prefer (e.g., Davlan/afro-xlmr-base)\n",
        "\n",
        "data = [json.loads(line) for line in open('/content/prepared/data.jsonl', encoding='utf-8')]\n",
        "dataset = Dataset.from_list([{'tokens': d['tokens'], 'tags': d['tags']} for d in data])\n",
        "\n",
        "unique_labels = sorted({lab for d in data for lab in d['tags']})\n",
        "label2id = {l:i for i,l in enumerate(unique_labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "print('Labels:', unique_labels)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, truncation=True, padding='max_length', max_length=128)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label[word_idx]])\n",
        "            else:\n",
        "                lab = label[word_idx]\n",
        "                if lab.startswith('B-'):\n",
        "                    lab = 'I-' + lab.split('-',1)[1]\n",
        "                label_ids.append(label2id.get(lab, label2id[label[word_idx]]))\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized = tokenized.train_test_split(test_size=0.1)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_labels), id2label=id2label, label2id=label2id)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='/content/nyahoba_ner_output',\n",
        "    eval_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['test'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print('Starting transformer training (this may take a long time).')\n",
        "trainer.train()\n",
        "trainer.save_model('/content/prepared/nyahoba-ner-model')\n",
        "print('Saved transformer model to /content/prepared/nyahoba-ner-model')"
      ],
      "id": "817afc7c",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels: ['B-ANIMAL', 'B-LOCATION', 'B-PERSON', 'B-TIME', 'I-ANIMAL', 'I-LOCATION', 'I-PERSON', 'I-TIME']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f75a5ac82ffe4a22a533e4ccf8bea560",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1600309562.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting transformer training (this may take a long time).\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchahyaandida\u001b[0m (\u001b[33mchahyaandida-modibbo-adama-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.0s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_105738-de2yi26i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface/runs/de2yi26i' target=\"_blank\">swift-terrain-1</a></strong> to <a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface' target=\"_blank\">https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface/runs/de2yi26i' target=\"_blank\">https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface/runs/de2yi26i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='796' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 796/2700 2:46:49 < 6:40:03, 0.08 it/s, Epoch 0.88/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='856' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 856/2700 2:59:24 < 6:27:22, 0.08 it/s, Epoch 0.95/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abec46a",
      "metadata": {
        "id": "3abec46a"
      },
      "source": [
        "## 7. Inference Examples\n",
        "Load saved models and run inference on sample texts. Edit the sample sentences as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0705318c",
      "metadata": {
        "id": "0705318c"
      },
      "outputs": [],
      "source": [
        "# CRF inference\n",
        "import joblib, json\n",
        "from pathlib import Path\n",
        "crf_path = Path('/content/prepared/crf_nyahoba.joblib')\n",
        "if crf_path.exists():\n",
        "    crf = joblib.load(crf_path)\n",
        "    sample = ['Ngala', 'ta', 'yana', 'kasuwa', '.']\n",
        "    def sent2features(sent):\n",
        "        def word2features(sent, i):\n",
        "            word = sent[i]\n",
        "            features = {\n",
        "                'bias': 1.0,\n",
        "                'word.lower()': word.lower(),\n",
        "                'word.isupper()': word.isupper(),\n",
        "                'word.istitle()': word.istitle(),\n",
        "                'word.isdigit()': word.isdigit(),\n",
        "            }\n",
        "            if i > 0:\n",
        "                word1 = sent[i-1]\n",
        "                features.update({\n",
        "                    '-1:word.lower()': word1.lower(),\n",
        "                    '-1:word.istitle()': word1.istitle(),\n",
        "                    '-1:word.isupper()': word1.isupper(),\n",
        "                })\n",
        "            else:\n",
        "                features['BOS'] = True\n",
        "            if i < len(sent)-1:\n",
        "                word1 = sent[i+1]\n",
        "                features.update({\n",
        "                    '+1:word.lower()': word1.lower(),\n",
        "                    '+1:word.istitle()': word1.istitle(),\n",
        "                    '+1:word.isupper()': word1.isupper(),\n",
        "                })\n",
        "            else:\n",
        "                features['EOS'] = True\n",
        "            return features\n",
        "        return [word2features(sent, i) for i in range(len(sent))]\n",
        "    print('CRF prediction:', crf.predict([sent2features(sample)]))\n",
        "else:\n",
        "    print('CRF model not found at', crf_path)\n",
        "\n",
        "# Transformer inference (if model exists)\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "model_dir = Path('/content/prepared/nyahoba-ner-model')\n",
        "if model_dir.exists():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
        "    nlp = pipeline('token-classification', model=model, tokenizer=tokenizer, aggregation_strategy='simple')\n",
        "    print(nlp('Ngala ta yana kasuwa.'))\n",
        "else:\n",
        "    print('Transformer model not found at', model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f2a25c",
      "metadata": {
        "id": "a6f2a25c"
      },
      "source": [
        "## 8. Save & Export\n",
        "- Use `File -> Download .ipynb` in Colab to download the notebook.\n",
        "- Download trained artifacts from `/content/prepared` (you can zip and download them in Colab)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f75a5ac82ffe4a22a533e4ccf8bea560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db24d48f370b4d4e9b92eb9322d882cd",
              "IPY_MODEL_d9fe824d85ab41a58737325e7f9ffbf3",
              "IPY_MODEL_dd2ed684fdee49879a57fe0f39e7f02c"
            ],
            "layout": "IPY_MODEL_f2c9c17d733d4ef89b07af34457e5bd2"
          }
        },
        "db24d48f370b4d4e9b92eb9322d882cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ed8add509e4103b94cb9b64bb76206",
            "placeholder": "​",
            "style": "IPY_MODEL_9d7b85c944534646a69f679b6671c828",
            "value": "Map: 100%"
          }
        },
        "d9fe824d85ab41a58737325e7f9ffbf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ec36e2ea5c41229e1926741a238dfe",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0db8ceef719a4b948d46a87ccc5ee440",
            "value": 8000
          }
        },
        "dd2ed684fdee49879a57fe0f39e7f02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e030f64252f94d3ea68a80e758ef6999",
            "placeholder": "​",
            "style": "IPY_MODEL_64a3ce25324c426a9c06deeb4b0a9247",
            "value": " 8000/8000 [00:01&lt;00:00, 6140.40 examples/s]"
          }
        },
        "f2c9c17d733d4ef89b07af34457e5bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ed8add509e4103b94cb9b64bb76206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d7b85c944534646a69f679b6671c828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0ec36e2ea5c41229e1926741a238dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db8ceef719a4b948d46a87ccc5ee440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e030f64252f94d3ea68a80e758ef6999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64a3ce25324c426a9c06deeb4b0a9247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}