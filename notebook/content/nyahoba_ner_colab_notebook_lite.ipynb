{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Nya-Hoba NER — Colab Notebook (Lite Version)\n",
        "---\n",
        "\n",
        "**Named Entity Recognition for Low-Resource African Languages: A Transformer-Based Case Study on Nya-Hoba**\n",
        "\n",
        "**Owner:** Chahyaandida Ishaya\n",
        "\n",
        "---\n",
        "Note: This notebook is optimized for Google Colab free tier. It uses a smaller model (`Davlan/afro-xlmr-mini`) and lighter settings for faster training.\n"
      ],
      "metadata": {
        "id": "v3H4zbTFzB-Y"
      },
      "id": "v3H4zbTFzB-Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROJECT OBJECTIVES\n",
        "---\n",
        "\n",
        "- **General Objective:** Design and evaluate a transformer-based NER system for Nya-Hoba.\n",
        "- **Specific Objectives:**\n",
        "  1. Collect, clean, and annotate a Nya-Hoba text corpus for NER tasks.\n",
        "  2. Develop baseline NER models using traditional machine learning approaches for benchmarking.\n",
        "  3. Fine-tune transformer-based models for NER on Nya-Hoba.\n",
        "  4. Evaluate model performance using Precision, Recall, and F1-score.\n",
        "  5. Release an open-source dataset and pre-trained models.\n",
        "---"
      ],
      "metadata": {
        "id": "mAXfUISc0NSZ"
      },
      "id": "mAXfUISc0NSZ"
    },
    {
      "cell_type": "markdown",
      "id": "5ab553a9",
      "metadata": {
        "id": "5ab553a9"
      },
      "source": [
        "## **1. Setup**\n",
        "Run the code cell to install required packages. On Colab this may take a few minutes.\n",
        "\n",
        "You may want to manually install a CUDA-compatible `torch` build if you intend to use GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e1cbfea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1cbfea5",
        "outputId": "07a2a245-d1f7-4872-e76a-c49049f9227c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn 1.6.1\n",
            "pandas 2.2.2\n",
            "transformers 4.56.1\n",
            "torch 2.8.0+cu126 CUDA: False\n"
          ]
        }
      ],
      "source": [
        "!pip install -q scikit-learn sklearn-crfsuite pandas joblib transformers datasets seqeval torch\n",
        "import sklearn, pandas, joblib, transformers, torch\n",
        "print('sklearn', sklearn.__version__)\n",
        "print('pandas', pandas.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('torch', torch.__version__, 'CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Generation & Cleaning**\n",
        " This stage expands small seed lists of entity words ( names, times, animals, and locations) into larger synthetic datasets through controlled randomization and pattern-based augmentation."
      ],
      "metadata": {
        "id": "gSRTWXVU3vdZ"
      },
      "id": "gSRTWXVU3vdZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "#Making directory for data\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "print('Your Data is located in /content/data/dataset.conll')\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 1. Fixed lists\n",
        "# ---------------------------\n",
        "time_words = [\n",
        "    \"Zǝkǝu\", \"Sakana\", \"ǝna\", \"ǝhna\", \"Pǝshinda\", \"Pishinda\",\n",
        "    \"Fer pǝchingǝ zekeu\", \"Pǝr pǝchingǝ zekeu\", \"Pǝchi\", \"Hya\"\n",
        "]\n",
        "\n",
        "animal_words = [\n",
        "    \"Kwa\", \"Mabǝlang\", \"Tǝga\", \"Gwanba\", \"Ha'l\", \"Dlǝgwam\", \"Thla\",\n",
        "    \"Kǝtǝn\", \"Chiwar\", \"Lǝvari\", \"Mapǝla'u\", \"Litsa\"\n",
        "]\n",
        "\n",
        "person_words_fixed = [\n",
        "    \"Chahyaandida\", \"Chabiya\", \"Hyellama\", \"Hyelnaya\", \"Wandiya\", \"Hyel\", \"Yesu\",\n",
        "    \"Chataimada\", \"Chatramada\", \"Nanunaya\", \"Mapida\", \"Shimbal\", \"Chai\",\n",
        "    \"Hyellachardati\", \"Hyellachardati\", \"Wamanyi\", \"Miyaninyi\", \"Miyakindahyelni\", \"Miyaninyi\"\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 2. Synthetic PERSON names\n",
        "# ---------------------------\n",
        "base_names = [\n",
        "    \"Abubakar\", \"Ibrahim\", \"Musa\", \"Usman\", \"Kabiru\", \"Bello\", \"Suleiman\",\n",
        "    \"Ahmad\", \"Aliyu\", \"Shehu\", \"Aminu\", \"Habiba\", \"Fatima\", \"Aisha\", \"Zainab\", \"Hauwa\",\n",
        "    \"Ruqayya\", \"Maryam\", \"Khadija\", \"Sa'adatu\", \"Yakubu\", \"Ismaila\", \"Nasiru\", \"Idris\",\n",
        "    \"John\", \"Paul\", \"Peter\", \"James\", \"Joseph\", \"Stephen\", \"Samuel\",\n",
        "    \"David\", \"Daniel\", \"Thomas\", \"Andrew\", \"Philip\", \"Simon\", \"Nathaniel\",\n",
        "    \"Grace\", \"Joyce\", \"Ruth\", \"Esther\", \"Naomi\", \"Sarah\", \"Deborah\",\n",
        "    \"Ndyako\", \"Pwakina\", \"Gargam\", \"Kwada\", \"Tizhe\", \"Lazarus\", \"Kwapre\",\n",
        "    \"Nzoka\", \"Jauro\", \"Birma\", \"Fwa\", \"Tumba\", \"Dlama\", \"Nuhu\", \"Zira\", \"Bitrus\",\n",
        "    \"Vandi\", \"Nggada\", \"Gimba\", \"Danjuma\"\n",
        "]\n",
        "\n",
        "prefixes = [\"Alhaji\", \"Malam\", \"Doctor\", \"Pastor\", \"Chief\", \"Prince\", \"Princess\", \"Rev\"]\n",
        "suffixes = [\"Abubakar\", \"Musa\", \"Ibrahim\", \"Aliyu\", \"Yakubu\", \"Bitrus\", \"Danjuma\", \"Zira\", \"Vandi\", \"Nuhu\"]\n",
        "syllables = [\"Nga\", \"Fwa\", \"Tiz\", \"Lam\", \"Bok\", \"Ngu\", \"Pwa\", \"Kiri\", \"Shaf\", \"Loru\", \"Baga\", \"Dla\", \"Hoba\", \"Zar\", \"Yam\", \"Kwada\"]\n",
        "\n",
        "def make_variants(base_list, prefixes, suffixes, syllables, target=2000, max_attempts=20000):\n",
        "    items = set(base_list)\n",
        "    attempts = 0\n",
        "    while len(items) < target and attempts < max_attempts:\n",
        "        r = random.random()\n",
        "        if r < 0.3 and prefixes:\n",
        "            new = random.choice(prefixes) + \" \" + random.choice(base_list)\n",
        "        elif r < 0.6 and suffixes:\n",
        "            new = random.choice(base_list) + \" \" + random.choice(suffixes)\n",
        "        elif r < 0.8 and syllables:\n",
        "            new = random.choice(syllables) + random.choice(syllables)\n",
        "        else:\n",
        "            new = random.choice(base_list) + \" \" + random.choice(base_list)\n",
        "        items.add(new)\n",
        "        attempts += 1\n",
        "\n",
        "    # Fill with duplicates if still short\n",
        "    items = list(items)\n",
        "    while len(items) < target:\n",
        "        items.append(random.choice(items))\n",
        "    return items[:target]\n",
        "\n",
        "random.seed(2025)\n",
        "all_person_names = make_variants(base_names + person_words_fixed, prefixes, suffixes, syllables, 2000)\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 2b. Expand TIME and ANIMAL with variants to 2000\n",
        "# ---------------------------\n",
        "time_prefixes = [\"Early\", \"Late\", \"Mid\", \"Pre\", \"Post\"]\n",
        "time_suffixes = [\"time\", \"hour\", \"day\", \"night\", \"season\"]\n",
        "time_syllables = [\"Zi\", \"Sa\", \"Na\", \"Ku\", \"Lo\", \"Mi\", \"Ta\"]\n",
        "\n",
        "animal_prefixes = [\"Wild\", \"Big\", \"Little\", \"Young\", \"Old\"]\n",
        "animal_suffixes = [\"beast\", \"cub\", \"ling\", \"hunter\", \"creature\"]\n",
        "animal_syllables = [\"Ka\", \"Mo\", \"La\", \"Ti\", \"Ro\", \"Zu\", \"Ba\"]\n",
        "\n",
        "all_time_words = make_variants(time_words, time_prefixes, time_suffixes, time_syllables, 2000)\n",
        "all_animal_words = make_variants(animal_words, animal_prefixes, animal_suffixes, animal_syllables, 2000)\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 3. Location generator\n",
        "# ---------------------------\n",
        "base_places = [\n",
        "    \"Yola\", \"Jimeta\", \"Numan\", \"Ganye\", \"Gombi\", \"Hong\", \"Mubi\", \"Michika\", \"Madagali\",\n",
        "    \"Maiha\", \"Fufore\", \"Song\", \"Demsa\", \"Guyuk\", \"Jada\", \"Lamurde\", \"Mayo-Belwa\",\n",
        "    \"Shelleng\", \"Toungo\", \"Pella\", \"Uba\", \"Dirma\", \"Holma\", \"Kala'a\", \"Garkida\",\n",
        "    \"Borrong\", \"Mayo-Lope\", \"Shuwa\", \"Mayo-Balewa\", \"River Benue\", \"Mayo Ine\",\n",
        "    \"Mayo Nguli\", \"Mayo Sanzu\", \"Kiri Dam\", \"Mandara Mountains\", \"Zumo Hill\", \"Fali Hills\"\n",
        "]\n",
        "\n",
        "prefixes_loc = [\"New\", \"Old\", \"Upper\", \"Lower\", \"North\", \"South\", \"East\", \"West\", \"Mayo\", \"Wuro\", \"Gidan\", \"Bari\"]\n",
        "suffixes_loc = [\"Gari\", \"Ward\", \"Hill\", \"Village\", \"Settlement\", \"Bridge\", \"Camp\", \"Market\", \"River\", \"Valley\", \"Peak\", \"Forest\", \"Reserve\", \"Dam\"]\n",
        "syllables_loc = [\"Kwa\", \"Ngu\", \"Mayo\", \"Zar\", \"Kiri\", \"Wuro\", \"Tula\", \"Nguwa\", \"Ganye\", \"Song\", \"Lam\", \"Mubi\", \"Pella\", \"Hoba\", \"Beli\", \"Tambo\", \"Shaf\", \"Loru\", \"Baga\", \"Zumo\"]\n",
        "\n",
        "all_places = make_variants(base_places, prefixes_loc, suffixes_loc, syllables_loc, 2000)\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 4. Annotation helper\n",
        "# ---------------------------\n",
        "def make_annotation(word, label):\n",
        "    return {\n",
        "        \"data\": {\"text\": word},\n",
        "        \"annotations\": [{\n",
        "            \"result\": [{\n",
        "                \"value\": {\n",
        "                    \"start\": 0,\n",
        "                    \"end\": len(word),\n",
        "                    \"text\": word,\n",
        "                    \"labels\": [label]\n",
        "                },\n",
        "                \"from_name\": \"label\",\n",
        "                \"to_name\": \"text\",\n",
        "                \"type\": \"labels\"\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "\n",
        "# Build datasets\n",
        "time_tasks = [make_annotation(w, \"TIME\") for w in all_time_words]          # expanded 2000\n",
        "animal_tasks = [make_annotation(w, \"ANIMAL\") for w in all_animal_words]    # expanded 2000\n",
        "person_tasks = [make_annotation(w, \"PERSON\") for w in all_person_names]    # expanded 2000\n",
        "location_tasks = [make_annotation(loc, \"LOCATION\") for loc in all_places]  # expanded 2000\n",
        "\n",
        "# ---------------------------\n",
        "# STEP 5. Merge datasets\n",
        "# ---------------------------\n",
        "merged = time_tasks + animal_tasks + person_tasks + location_tasks\n",
        "\n",
        "with open(\"/content/data/merged_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(merged, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Saved {len(merged)} tasks -> merged_dataset.json\")\n",
        "print(f\"  TIME: {len(time_tasks)}\")\n",
        "print(f\"  ANIMAL: {len(animal_tasks)}\")\n",
        "print(f\"  PERSON: {len(person_tasks)}\")\n",
        "print(f\"  LOCATION: {len(location_tasks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLshArnt3rF0",
        "outputId": "4953ae0a-83db-4092-d57e-57d45e3b3bc9"
      },
      "id": "SLshArnt3rF0",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Data is located in /content/data/dataset.conll\n",
            "✅ Saved 8000 tasks -> merged_dataset.json\n",
            "  TIME: 2000\n",
            "  ANIMAL: 2000\n",
            "  PERSON: 2000\n",
            "  LOCATION: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Data Annotation**\n",
        " The generated data is then annotated with entity labels and merged into a unified dataset, ensuring sufficient volume and diversity for NER model training while maintaining consistency and quality."
      ],
      "metadata": {
        "id": "cR8ROXDq_n_4"
      },
      "id": "cR8ROXDq_n_4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your merged dataset\n",
        "with open(\"/content/data/merged_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "conll_lines = []\n",
        "\n",
        "for task in data:\n",
        "    text = task[\"data\"][\"text\"]\n",
        "    anns = task[\"annotations\"][0][\"result\"] if task[\"annotations\"] else []\n",
        "\n",
        "    # Start with \"O\" for each token\n",
        "    tokens = text.split()\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    for ann in anns:\n",
        "        value = ann[\"value\"]\n",
        "        start = value[\"start\"]\n",
        "        end = value[\"end\"]\n",
        "        label = value[\"labels\"][0]\n",
        "\n",
        "        # Find which tokens are covered by this annotation\n",
        "        covered = []\n",
        "        running_index = 0\n",
        "        for i, tok in enumerate(tokens):\n",
        "            token_start = running_index\n",
        "            token_end = running_index + len(tok)\n",
        "            if token_end > start and token_start < end:\n",
        "                covered.append(i)\n",
        "            running_index = token_end + 1  # +1 for space\n",
        "\n",
        "        # Assign BIO tags\n",
        "        for j, idx in enumerate(covered):\n",
        "            if j == 0:\n",
        "                labels[idx] = \"B-\" + label\n",
        "            else:\n",
        "                labels[idx] = \"I-\" + label\n",
        "\n",
        "    # Append tokens with tags\n",
        "    for tok, lab in zip(tokens, labels):\n",
        "        conll_lines.append(f\"{tok} {lab}\")\n",
        "    conll_lines.append(\"\")  # Sentence boundary\n",
        "\n",
        "# Save to file\n",
        "with open(\"/content/data/dataset.conll\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(conll_lines))\n",
        "\n",
        "print(\"✅ Exported to dataset.conll in CoNLL format\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPr01hmp_hQw",
        "outputId": "35e1378e-2ccb-4ec4-f12a-39c9ea7af72a"
      },
      "id": "GPr01hmp_hQw",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Exported to dataset.conll in CoNLL format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b56e2ab6",
      "metadata": {
        "id": "b56e2ab6"
      },
      "source": [
        "## 4. Parse CoNLL & Prepare JSONL\n",
        "This cell parses the CoNLL file (token per line, tag in last column) and saves a JSONL to `/content/prepared/data.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "156f4f05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "156f4f05",
        "outputId": "3bbda2e0-686d-477c-e6c0-4e9e06460795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: 8000 Tokens: 15735\n",
            "Saved prepared data to /content/prepared/data.jsonl\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "conll_path = Path('/content/data/dataset.conll')\n",
        "if not conll_path.exists():\n",
        "    raise FileNotFoundError('Upload dataset.conll to /content/data/ first.')\n",
        "\n",
        "def read_conll(path):\n",
        "    sentences, tokens, tags = [], [], []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if tokens: sentences.append((tokens, tags)); tokens, tags = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            tokens.append(parts[0]); tags.append(parts[-1])\n",
        "        if tokens: sentences.append((tokens, tags))\n",
        "    return sentences\n",
        "\n",
        "sentences = read_conll(conll_path)\n",
        "print('Sentences:', len(sentences), 'Tokens:', sum(len(s[0]) for s in sentences))\n",
        "\n",
        "os.makedirs('/content/prepared', exist_ok=True)\n",
        "with open('/content/prepared/data.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for tks, tgs in sentences:\n",
        "        f.write(json.dumps({'tokens': tks, 'tags': tgs}) + '\\n')\n",
        "print('Saved prepared data to /content/prepared/data.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Sample annotated sentences\n",
        "List of annotated sentences"
      ],
      "metadata": {
        "id": "TvJn4u0C1Sgv"
      },
      "id": "TvJn4u0C1Sgv"
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 10 samples\n",
        "import json, itertools\n",
        "with open('/content/prepared/data.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(itertools.islice(f, 10), 1):\n",
        "        d = json.loads(line)\n",
        "        print(i, '->', ' '.join([f\"{t}/{tg}\" for t,tg in zip(d['tokens'], d['tags'])]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKnntAsm1VeQ",
        "outputId": "02ea8fab-b2b6-4642-a8a1-d0287bdd9cfd"
      },
      "id": "gKnntAsm1VeQ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 -> ǝhna/B-TIME Hya/I-TIME\n",
            "2 -> Pǝshinda/B-TIME Pǝr/I-TIME pǝchingǝ/I-TIME zekeu/I-TIME\n",
            "3 -> KuMi/B-TIME\n",
            "4 -> Pǝchi/B-TIME Pishinda/I-TIME\n",
            "5 -> Mid/B-TIME Zǝkǝu/I-TIME\n",
            "6 -> Pǝr/B-TIME pǝchingǝ/I-TIME zekeu/I-TIME Sakana/I-TIME\n",
            "7 -> Early/B-TIME Pǝchi/I-TIME\n",
            "8 -> Sakana/B-TIME Zǝkǝu/I-TIME\n",
            "9 -> Zǝkǝu/B-TIME night/I-TIME\n",
            "10 -> Hya/B-TIME Pǝchi/I-TIME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a43791",
      "metadata": {
        "id": "43a43791"
      },
      "source": [
        "## 6. Baseline: CRF Model\n",
        "Train a CRF baseline using `sklearn-crfsuite`. This step is fast and useful for benchmarking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0824a1b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0824a1b2",
        "outputId": "9832e372-807d-468a-b859-ea1dc3f151d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    B-ANIMAL       1.00      0.99      0.99       379\n",
            "  B-LOCATION       0.88      1.00      0.94       399\n",
            "    B-PERSON       1.00      0.89      0.94       397\n",
            "      B-TIME       1.00      0.99      0.99       425\n",
            "    I-ANIMAL       1.00      1.00      1.00       311\n",
            "  I-LOCATION       1.00      1.00      1.00       427\n",
            "    I-PERSON       1.00      1.00      1.00       339\n",
            "      I-TIME       1.00      1.00      1.00       486\n",
            "\n",
            "    accuracy                           0.98      3163\n",
            "   macro avg       0.99      0.98      0.98      3163\n",
            "weighted avg       0.98      0.98      0.98      3163\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/prepared/crf_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from sklearn_crfsuite import CRF, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "data = [json.loads(l) for l in open('/content/prepared/data.jsonl')]\n",
        "tokens = [d['tokens'] for d in data]\n",
        "tags = [d['tags'] for d in data]\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    feats = {'bias':1.0,'word.lower()':word.lower(),'word.isupper()':word.isupper(),\n",
        "             'word.istitle()':word.istitle(),'word.isdigit()':word.isdigit()}\n",
        "    if i>0: feats.update({'-1:word.lower()':sent[i-1].lower()})\n",
        "    else: feats['BOS']=True\n",
        "    if i<len(sent)-1: feats.update({'+1:word.lower()':sent[i+1].lower()})\n",
        "    else: feats['EOS']=True\n",
        "    return feats\n",
        "\n",
        "X = [[word2features(s,i) for i in range(len(s))] for s in tokens]\n",
        "y = tags\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "\n",
        "crf = CRF(max_iterations=100)\n",
        "crf.fit(X_train,y_train)\n",
        "y_pred = crf.predict(X_test)\n",
        "print(metrics.flat_classification_report(y_test,y_pred))\n",
        "joblib.dump(crf,'/content/prepared/crf_model.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4cde24",
      "metadata": {
        "id": "4f4cde24"
      },
      "source": [
        "## 7. Transformer Fine-Tuning (Lite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986ce99c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526,
          "referenced_widgets": [
            "5ec53662fbbf403aaa3e799a49fa00b0",
            "443d8f3043d44c309386d6678239adf8",
            "142a23728d7a445ab923df8bd9188ad1",
            "b131cf3daefc40499b5624d64d80bf8e",
            "42ea38539bcc4feeae34507211e933d0",
            "0475d01918d54547b7eaccdb42f7c518",
            "fa3f0393a3544a83b364deb567ee44af",
            "ae455d10c6eb40c08cf34ceb2a3e37d3",
            "8fcf46f6e8454e7dac29dd09ba83881e",
            "9e9a81d69ecb4f82b9beaddb4bc2e88b",
            "073df37f726c43c59eac0867aad04312"
          ]
        },
        "id": "986ce99c",
        "outputId": "c36980e8-e7a0-4dc9-af2a-47ec0e6e492b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ec53662fbbf403aaa3e799a49fa00b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-4016157348.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchahyaandida\u001b[0m (\u001b[33mchahyaandida-modibbo-adama-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (0.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250912_131035-ovpwtc22</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface/runs/ovpwtc22' target=\"_blank\">dauntless-bush-2</a></strong> to <a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface' target=\"_blank\">https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface/runs/ovpwtc22' target=\"_blank\">https://wandb.ai/chahyaandida-modibbo-adama-university/huggingface/runs/ovpwtc22</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  21/1800 01:33 < 2:26:06, 0.20 it/s, Epoch 0.02/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "\n",
        "MODEL_NAME = \"Davlan/afro-xlmr-mini\"\n",
        "\n",
        "data = [json.loads(l) for l in open('/content/prepared/data.jsonl')]\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "labels = sorted({lab for d in data for lab in d['tags']})\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize_and_align(examples):\n",
        "    tok = tokenizer(examples['tokens'], is_split_into_words=True, truncation=True, padding='max_length', max_length=128)\n",
        "    new_labels = []\n",
        "    for i, labs in enumerate(examples['tags']):\n",
        "        word_ids = tok.word_ids(batch_index=i)\n",
        "        prev, label_ids = None, []\n",
        "        for wid in word_ids:\n",
        "            if wid is None: label_ids.append(-100)\n",
        "            elif wid != prev: label_ids.append(label2id[labs[wid]])\n",
        "            else: label_ids.append(label2id[labs[wid]] if labs[wid].startswith('I-') else label2id[labs[wid].replace('B-','I-')])\n",
        "            prev = wid\n",
        "        new_labels.append(label_ids)\n",
        "    tok['labels'] = new_labels\n",
        "    return tok\n",
        "\n",
        "tokenized = dataset.map(tokenize_and_align, batched=True)\n",
        "tokenized = tokenized.train_test_split(test_size=0.1)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='/content/ner_out',\n",
        "    eval_strategy='epoch',\n",
        "    eval_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=20,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['test'],\n",
        "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model('/content/prepared/ner_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec54fcf",
      "metadata": {
        "id": "aec54fcf"
      },
      "source": [
        "## 8. Inference Examples\n",
        "Load saved models and run inference on sample texts. Edit the sample sentences as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bad7559",
      "metadata": {
        "id": "8bad7559"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
        "\n",
        "model_dir = '/content/prepared/ner_model'\n",
        "if os.path.exists(model_dir):\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "    mod = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
        "    nlp = pipeline('token-classification', model=mod, tokenizer=tok, aggregation_strategy='simple')\n",
        "    print(nlp(\"Ngala ta yana kasuwa.\"))\n",
        "else:\n",
        "    print(\"Train the model first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ec53662fbbf403aaa3e799a49fa00b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_443d8f3043d44c309386d6678239adf8",
              "IPY_MODEL_142a23728d7a445ab923df8bd9188ad1",
              "IPY_MODEL_b131cf3daefc40499b5624d64d80bf8e"
            ],
            "layout": "IPY_MODEL_42ea38539bcc4feeae34507211e933d0"
          }
        },
        "443d8f3043d44c309386d6678239adf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0475d01918d54547b7eaccdb42f7c518",
            "placeholder": "​",
            "style": "IPY_MODEL_fa3f0393a3544a83b364deb567ee44af",
            "value": "Map: 100%"
          }
        },
        "142a23728d7a445ab923df8bd9188ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae455d10c6eb40c08cf34ceb2a3e37d3",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fcf46f6e8454e7dac29dd09ba83881e",
            "value": 8000
          }
        },
        "b131cf3daefc40499b5624d64d80bf8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9a81d69ecb4f82b9beaddb4bc2e88b",
            "placeholder": "​",
            "style": "IPY_MODEL_073df37f726c43c59eac0867aad04312",
            "value": " 8000/8000 [00:01&lt;00:00, 5955.77 examples/s]"
          }
        },
        "42ea38539bcc4feeae34507211e933d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0475d01918d54547b7eaccdb42f7c518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa3f0393a3544a83b364deb567ee44af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae455d10c6eb40c08cf34ceb2a3e37d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcf46f6e8454e7dac29dd09ba83881e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9a81d69ecb4f82b9beaddb4bc2e88b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "073df37f726c43c59eac0867aad04312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}